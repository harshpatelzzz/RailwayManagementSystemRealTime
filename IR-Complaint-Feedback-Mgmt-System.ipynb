{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Indian Railways Complaint & Feedback Management System\n",
        "\n",
        "## Data Analysis and Model Development\n",
        "\n",
        "This notebook contains the analysis of Indian Railways tweets and development of the classification model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Explore Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load training data\n",
        "# Replace with your actual data path\n",
        "try:\n",
        "    df = pd.read_csv('data/training_data.csv')\n",
        "    print(f\"Data loaded successfully. Shape: {df.shape}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(df.head())\n",
        "except FileNotFoundError:\n",
        "    print(\"Training data file not found. Creating sample data...\")\n",
        "    # Create sample data\n",
        "    sample_data = {\n",
        "        'tweet': [\n",
        "            \"Train is delayed urgent help needed\",\n",
        "            \"Excellent service on railway\",\n",
        "            \"Accident on track emergency\",\n",
        "            \"Thank you for good service\",\n",
        "            \"Train breakdown help required\",\n",
        "            \"Comfortable journey\",\n",
        "            \"Medical emergency in train\",\n",
        "            \"Clean and tidy coach\",\n",
        "            \"Train cancelled need refund\",\n",
        "            \"Great food quality\"\n",
        "        ],\n",
        "        'label': [1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n",
        "    }\n",
        "    df = pd.DataFrame(sample_data)\n",
        "    print(f\"Sample data created. Shape: {df.shape}\")\n",
        "    print(\"\\nFirst few rows:\")\n",
        "    print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_tweet(text):\n",
        "    \"\"\"Clean tweet text\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Remove user mentions\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    \n",
        "    # Remove special characters but keep spaces\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    return text.lower().strip()\n",
        "\n",
        "# Apply cleaning\n",
        "df['cleaned_tweet'] = df['tweet'].apply(clean_tweet)\n",
        "print(\"Data cleaned successfully!\")\n",
        "print(\"\\nSample cleaned tweets:\")\n",
        "print(df[['tweet', 'cleaned_tweet']].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class distribution\n",
        "print(\"Class Distribution:\")\n",
        "print(df['label'].value_counts())\n",
        "print(f\"\\nEmergency (1): {df['label'].sum()}\")\n",
        "print(f\"Feedback (0): {(df['label'] == 0).sum()}\")\n",
        "\n",
        "# Visualize class distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "df['label'].value_counts().plot(kind='bar', color=['skyblue', 'coral'])\n",
        "plt.title('Class Distribution')\n",
        "plt.xlabel('Class (0=Feedback, 1=Emergency)')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Extraction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
        "X = vectorizer.fit_transform(df['cleaned_tweet'])\n",
        "y = df['label']\n",
        "\n",
        "print(f\"Feature matrix shape: {X.shape}\")\n",
        "print(f\"Target vector shape: {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Model Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nModel trained successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Feedback', 'Emergency']))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "            xticklabels=['Feedback', 'Emergency'],\n",
        "            yticklabels=['Feedback', 'Emergency'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create saved_model directory if it doesn't exist\n",
        "os.makedirs('saved_model', exist_ok=True)\n",
        "\n",
        "# Save model and vectorizer\n",
        "joblib.dump(model, 'saved_model/tweet_classifier_model.pkl')\n",
        "joblib.dump(vectorizer, 'saved_model/tfidf_vectorizer.pkl')\n",
        "\n",
        "print(\"Model and vectorizer saved successfully!\")\n",
        "print(\"Files saved:\")\n",
        "print(\"  - saved_model/tweet_classifier_model.pkl\")\n",
        "print(\"  - saved_model/tfidf_vectorizer.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test Predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with sample tweets\n",
        "test_tweets = [\n",
        "    \"Train is delayed by 2 hours, urgent help needed\",\n",
        "    \"Great service, comfortable journey\",\n",
        "    \"Medical emergency in coach number 5\",\n",
        "    \"Thank you for the excellent food\"\n",
        "]\n",
        "\n",
        "for tweet in test_tweets:\n",
        "    cleaned = clean_tweet(tweet)\n",
        "    X_test_tweet = vectorizer.transform([cleaned])\n",
        "    prediction = model.predict(X_test_tweet)[0]\n",
        "    proba = model.predict_proba(X_test_tweet)[0]\n",
        "    \n",
        "    label = \"Emergency\" if prediction == 1 else \"Feedback\"\n",
        "    confidence = proba[prediction] * 100\n",
        "    \n",
        "    print(f\"\\nTweet: {tweet}\")\n",
        "    print(f\"Prediction: {label} (Confidence: {confidence:.2f}%)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
